{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfbd0c4c-c79e-4f1c-ad3d-7d242893fbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.13.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.5 MB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: wheel in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.5.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn) (1.24.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 297 kB 84.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.3.2\n",
      "  Downloading scipy-1.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34.4 MB 108.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.0 scipy-1.10.0 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install transformers datasets evaluate emojis\n",
    "# %pip install torch scikit-learn  # sagemaker studio LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97522057-adbe-4a6b-aef3-e578fa1ee085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.executable: /home/studio-lab-user/.conda/envs/default/bin/python\n",
      "sys.version   : 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50) [GCC 10.3.0]\n",
      "sys.path      :\n",
      "                /home/studio-lab-user/tamil/tamil_mix_sentiment_analysis\n",
      "                /home/studio-lab-user/.conda/envs/default/lib/python39.zip\n",
      "                /home/studio-lab-user/.conda/envs/default/lib/python3.9\n",
      "                /home/studio-lab-user/.conda/envs/default/lib/python3.9/lib-dynload\n",
      "                \n",
      "                /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages\n",
      "                /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/IPython/extensions\n",
      "                /home/studio-lab-user/.ipython\n",
      "cpu\n",
      "CPU\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import logging\n",
    "import evaluate\n",
    "\n",
    "import datasets\n",
    "from datasets import load_metric\n",
    "\n",
    "import emojis\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, balanced_accuracy_score, classification_report\n",
    "\n",
    "print('sys.executable:', sys.executable)\n",
    "print('sys.version   :', sys.version.replace('\\n', ''))\n",
    "print('sys.path      :')\n",
    "for x in sys.path:\n",
    "    print('               ', x)\n",
    "\n",
    "# os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda n:', torch.cuda.device_count())\n",
    "    current = torch.cuda.current_device()\n",
    "    print('current', current, torch.cuda.device(current))\n",
    "else:\n",
    "    print('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4407fd-5b3c-442f-a817-6089cbccfea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('tamilmixsentiment')\n",
    "type(dataset)\n",
    "dataset\n",
    "\n",
    "testset = dataset.pop('test')\n",
    "testset\n",
    "\n",
    "my_emojis = [list(emojis.get(x)) for x in dataset['train']['text']]\n",
    "my_emojis = [y for x in my_emojis for y in x]\n",
    "print(pd.Series(my_emojis).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c264d1e-870c-4fe7-9e7c-28af4f1258bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make subset of the data\n",
    "def red(ds):\n",
    "    p = 0.1\n",
    "    n = int(ds.num_rows * p)\n",
    "    res = ds.shuffle(seed=2023).select(range(n))\n",
    "    return res\n",
    "\n",
    "\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': red(dataset['train']),\n",
    "    'validation': red(dataset['validation']),\n",
    "})\n",
    "dataset\n",
    "\n",
    "print(dataset['train'].to_pandas()['label'].value_counts(normalize=True))\n",
    "print(dataset['validation'].to_pandas()['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e84f5-dfea-4261-a115-13f6b0a7ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-multilingual-cased'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "print(len(tokenizer))  # 119547\n",
    "assert tokenizer.tokenize('ðŸ§¡') == ['[UNK]']\n",
    "\n",
    "tokenizer.add_tokens(list(set(my_emojis)))\n",
    "print(len(tokenizer))  # 119593\n",
    "assert tokenizer.tokenize('ðŸ§¡') == ['ðŸ§¡']\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "num_labels = len(set(dataset['train']['label']))\n",
    "print('num_labels:', num_labels)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "model.num_labels\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print('model.device', model.device)\n",
    "model.to(device)\n",
    "print('model.device', model.device)\n",
    "\n",
    "\n",
    "def tokenize_function(ds):\n",
    "    res = tokenizer(\n",
    "        ds['text'],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        # max_length=512,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    return res\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset\n",
    "\n",
    "tokenized_dataset_train = tokenizer(\n",
    "        dataset['train']['text'],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        # max_length=512,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "tokenized_dataset_validation = tokenizer(\n",
    "        dataset['validation']['text'],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        # max_length=512,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "\n",
    "# tokenized_dataset_test = tokenizer(\n",
    "#         dataset['test']['text'],\n",
    "#         padding=True,\n",
    "#         truncation=True,\n",
    "#         # max_length=512,\n",
    "#         return_tensors='pt',\n",
    "#     )\n",
    "\n",
    "\n",
    "# dataset = datasets.DatasetDict({\n",
    "#     'train': red(dataset['train']),\n",
    "#     'validation': red(dataset['validation']),\n",
    "# })\n",
    "# dataset\n",
    "\n",
    "\n",
    "tokenized_dataset.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897f840-7b42-4434-9781-e06c8d06d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_train\n",
    "\n",
    "tokenized_dataset\n",
    "\n",
    "# tokenized_dataset.to('pytorch')\n",
    "# type(tokenized_dataset['train']['attention_mask'])\n",
    "\n",
    "# tokenized_dataset['train']['attention_mask']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db5c06-d51a-499b-8321-0ecdf13e8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    res = metric.compute(predictions=predictions, references=labels)\n",
    "    print('EVALUANDO', flush=True)\n",
    "    print(type(res), flush=True)\n",
    "    print(res, flush=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_bert',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    save_strategy='epoch',  # 'no'\n",
    "    evaluation_strategy='steps',  # 'no'\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "\n",
    "my_weights = 1 / dataset['train'].to_pandas()['label'].value_counts(normalize=True)\n",
    "my_weights = np.log(my_weights)\n",
    "my_weights = my_weights.tolist()\n",
    "my_weights\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get('labels')\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        # loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0]))\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor(my_weights))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    # train_dataset=tokenized_dataset_train,\n",
    "    # eval_dataset=tokenized_dataset_validation,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8fdb07-c260-43a7-a0e4-715ead1cd745",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trainer.args.device:', trainer.args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516b1ba-5143-43ee-87e2-f20b5ddee6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8612ab5-99a8-4931-b04e-b5aef34e442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./results_bert_final_trained_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b9e4b-d129-433e-86c7-43540030a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "ts = testset.to_pandas()\n",
    "\n",
    "pip = pipeline('sentiment-analysis', './results_bert_final_trained_2')\n",
    "\n",
    "res = pip(ts['text'].tolist())\n",
    "res\n",
    "ts[['pred', 'prob']] = pd.DataFrame(res).values\n",
    "ts\n",
    "ts['pred'].value_counts()\n",
    "ts['pred'].value_counts(normalize=True)\n",
    "\n",
    "ts['pred_n'] = ts['pred'].str.replace('LABEL_', '').astype(int)\n",
    "\n",
    "pd.crosstab(ts['label'], ts['pred'])\n",
    "pd.crosstab(ts['label'], ts['pred_n'])\n",
    "\n",
    "precision_recall_fscore_support(ts['label'], ts['pred_n'])\n",
    "pd.DataFrame(precision_recall_fscore_support(ts['label'], ts['pred_n']))\n",
    "\n",
    "precision_recall_fscore_support(ts['label'], ts['pred_n'], average='weighted')\n",
    "\n",
    "print(classification_report(ts['label'], ts['pred_n']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442153e-558d-4b53-9381-affc5231bd05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
